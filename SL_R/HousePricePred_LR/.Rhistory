head(df_features1)
head(df_features1)
#************** Reports count per project
featureset7_q = paste("select project_id, count(*) recport_cnt from redcap_reports rr where rr.project_id in (",project_ids,") group by rr.project_id",sep=" ")
rs = dbSendQuery(con,featureset7_q)
## Making connection to your MySQL db
con <- dbConnect(MySQL(), user="root", password="brl@ndHjN3w", dbname="redcap_new", host="127.0.0.1", port = 3306)
rs = dbSendQuery(con,featureset7_q)
df_features7 = fetch(rs, n=-1) # arm-event counts
nrow(df_features7)
##merging dataframes
df_features1 <- merge(x=df_features1,y=df_features7,by='project_id',all.x=TRUE)
head(df_features1)
#************** DAGS count per project
featureset8_q = paste("select project_id, count(*) as dag_cnt from redcap_data_access_groups dag where dag.project_id in (",project_ids,") group by dag.project_id",sep=" ")
rs = dbSendQuery(con,featureset8_q)
df_features8 = fetch(rs, n=-1) # arm-event counts
##merging dataframes
df_features1 <- merge(x=df_features1,y=df_features8,by='project_id',all.x=TRUE)
head(df_features1)
nrow(df_features8)
#************** Calendar entries count per project
featureset9_q = paste("select project_id, count(*) as calRecord_cnt from redcap_events_calendar rev where rev.project_id in (",project_ids,") group by rev.project_id",sep=" ")
rs = dbSendQuery(con,featureset9_q)
featureset9_q = fetch(rs, n=-1) # arm-event counts
##merging dataframes
df_features1 <- merge(x=df_features1,y=df_features9,by='project_id',all.x=TRUE)
df_features9 = fetch(rs, n=-1) # arm-event counts
##merging dataframes
df_features1 <- merge(x=df_features1,y=df_features9,by='project_id',all.x=TRUE)
head(df_features1)
#************** Bookmarks count per project
featureset10_q = paste("select project_id, count(*) as bookmarks_cnt from redcap_external_links rel where rel.project_id in (",project_ids,") group by rel.project_id",sep=" ")
rs = dbSendQuery(con,featureset10_q)
df_features10 = fetch(rs, n=-1) # arm-event counts
##merging dataframes
df_features1 <- merge(x=df_features1,y=df_features10,by='project_id',all.x=TRUE)
head(df_features1)
#************** File repository/file uploads/Signatures count per project
featureset11_q = paste("select project_id,count(*) as edocs_cnt from redcap_edocs_metadata edoc where edoc.project_id in (",project_ids,") group by edoc.project_id",sep=" ")
rs = dbSendQuery(con,featureset11_q)
## Making connection to your MySQL db
con <- dbConnect(MySQL(), user="root", password="brl@ndHjN3w", dbname="redcap_new", host="127.0.0.1", port = 3306)
#************** File repository/file uploads/Signatures count per project
featureset11_q = paste("select project_id,count(*) as edocs_cnt from redcap_edocs_metadata edoc where edoc.project_id in (",project_ids,") group by edoc.project_id",sep=" ")
rs = dbSendQuery(con,featureset11_q)
df_features11 = fetch(rs, n=-1) # arm-event counts
##merging dataframes
df_features1 <- merge(x=df_features11,y=df_features11,by='project_id',all.x=TRUE)
head(df_features1)
head(df_features11)
#************** API count per project
featureset12_q = paste("SELECT project_id, count(distinct(api_token)) from redcap_user_rights api where (api_token is not null and api.project_id in (",project_ids,")) group by api.project_id",sep=" ")
featureset12_q
featureset12_q -< ""
featureset12_q <- ""
#************** API count per project
featureset12_q = paste("SELECT project_id, count(distinct(api_token)) api_cnt from redcap_user_rights api where (api_token is not null and api.project_id in (",project_ids,")) group by api.project_id",sep=" ")
featureset12_q
#************** API count per project
featureset12_q = paste("SELECT project_id, count(distinct(api_token)) api_cnt from redcap_user_rights api where (api_token is not null and api.project_id in (",project_ids,")) group by api.project_id",sep=" ")
rs = dbSendQuery(con,featureset12_q)
df_features12 = fetch(rs, n=-1) # arm-event counts
##merging dataframes
df_features1 <- merge(x=df_features11,y=df_features12,by='project_id',all.x=TRUE)
head(df_features1)
#************** Record numbers in projects - The count of unique records
featureset13_q = paste("select project_id, count(distinct(record)) from redcap_data rd where rd.project_id in (",project_ids,") group by rd.project_id",sep=" ")
rs = dbSendQuery(con,featureset13_q)
#************** Record numbers in projects - The count of unique records
featureset13_q = paste("select project_id, count(distinct(record)) record_cnt from redcap_data rd where rd.project_id in (",project_ids,") group by rd.project_id",sep=" ")
featureset13_q
rs = dbSendQuery(con,featureset13_q)
df_features13 = fetch(rs, n=-1) # Record counts
#************** Record numbers in projects - The count of unique records
featureset13_q = paste("select project_id, count(distinct(record)) record_cnt from redcap_data rd where rd.project_id in (",project_ids,") group by rd.project_id",sep=" ")
rs = dbSendQuery(con,featureset13_q)
df_features13 = fetch(rs, n=-1) # Record counts
###merging dataframes
df_features13 <- merge(x=df_features1,y=df_features13,by='project_id',all.x=TRUE)
head(df_features1)
head(df_features13)
###merging dataframes
df_features1 <- merge(x=df_features1,y=df_features13,by='project_id',all.x=TRUE)
head(df_features1)
## Making connection to your MySQL db
con <- dbConnect(MySQL(), user="root", password="brl@ndHjN3w", dbname="redcap_new", host="127.0.0.1", port = 3306)
### if your db is hosted on server then you enter ip address of server in host
#************** query to get Still active projects
stillActive_q = "select a.project_id,b.project_name,max(a.ts) as lastLogEvent from redcap_projects as b left outer join redcap_log_event as a on (a.project_id=b.project_id and b.status in (0,1)) where lower(app_title) not like '%migrate%' group by project_id having max(ts)>20191231000000"
#************** E-Signatures count per project
featureset14_q = paste("select project_id,count(*) esignature_cnt from redcap_edocs_metadata edoc where (edoc.doc_name like 'signature%' and edoc.project_id in (",project_ids,")) group by edoc.project_id",sep=" ")
featureset14_q
timwstamp
timestamp()
##------ Fri Jan 22 09:41:47 2021 ------##
library(RMySQL)
library(RMySQL)
#redcap7
con <- dbConnect(MySQL(), user="root", password="brl@ndHjN3w", dbname="redcap", host="127.0.0.1", port = 3306)
stillActive_q = "select a.project_id,b.project_name,max(a.ts) as lastLogEvent from redcap_projects as b left outer join redcap_log_event as a on (a.project_id=b.project_id and b.status in (0,1)) where lower(app_title) not like '%migrate%' group by project_id having max(ts)>20191231000000"
#stillActive_q = "select * from redcap_data limit 5;"
timestamp()
##------ Fri Jan 22 09:44:19 2021 ------##
rs = dbSendQuery(con,stillActive_q)
timestamp()
##------ Fri Jan 22 09:56:03 2021 ------##
## fetching data from the query
data = fetch(rs, n=-1) # Still Active projects
library(dplyr)
library(log4r)
install.packages("~/DeepsDownloads/Rscripts_packages/logr_1.2.1.tar.gz", repos = NULL, type = "source")
library(logr)
library(logr)
install.packages(c("boot", "cluster", "MASS", "mgcv"))
View(data_grp_stateNStatus)
#Project 2 - comcast complaints analysis
### set working directory
#Can use this command if you have Rstudio IDE
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
### Libraries
library(logr)
library(lubridate) #for date parsing
library(dplyr)
library(ggplot2)  #visualization
library(reshape2) #pivot tble
#NLP packages
library(tm)  #Text mining package
#library(SnowballC) #collapsing word to common root-Stemming
library(RColorBrewer) #provide color schemes for maps
library(wordcloud)
### Set up log file
tmp <- file.path(dirname(rstudioapi::getActiveDocumentContext()$path), "SL_R_Proj3_Comcast.log")
# Open log
lf <- log_open(tmp)
# Send message to log
log_print("Simlilearn - DS with R Pgming - Project 2")
### Q.Import data in to R
data <- read.csv("../data/Comcast Telecom Complaints data.csv")
### To check missing values
#apply(is.na(data),2, which)
# No missing values found
### Functions
date_parsing <- function(dt) {
#print(dt)
if(!is.na(dt)){
for(date_format in c("%d/%m/%Y","%d-%m-%Y")){
datehead <- format(strptime(as.character(dt),date_format),"%Y-%m-%d")
if(is.na(datehead)){}
else{break}
}#end of for
}# end of if
return(datehead)
}
### Data cleaning - make dates in to same format
#str(data) # to get info about the dataframe columns. Here all cols are in chr format
data$newDate <- unlist(lapply(data$Date,date_parsing))
### Q.Plot data monthly
#create year month column
data$newDate <- as.Date(data$newDate)
#write.csv(data,'./log/temp.csv',row.names = F)
#group by month
data_grp_mth <- data %>%
group_by(month = lubridate::floor_date(newDate, "month")) %>%
summarize(cnt = n_distinct(Customer.Complaint))
#plot data
ggplot(data_grp_mth, aes(month, cnt)) +
geom_point() +
geom_line() +
xlab("Month") +
ylab("Number of Complaints") +
ggtitle("Monthly Comcast complaint frequency")
### Q.Plot data daily
#group by daily
data_grp_day <- data %>%
group_by(newDate) %>%
summarize(cnt = n_distinct(Customer.Complaint))
#plot data
ggplot(data_grp_day, aes(newDate, cnt)) +
geom_point() +
geom_line() +
xlab("Date") +
ylab("Number of Complaints") +
ggtitle("Daily Comcast complaint frequency")
### Observations ###
log_print("By looking at the daily and monthly plots, we can conclude that most number of complaints are added in June 2015.")
### Observations ###
### Q.Provide a table with the frequency of complaint types.
### Q.Which complaint types are maximum i.e., around internet, network issues, or across any other domains.
#Identify different complaint types from Customer.complaint column values and their count
#corpus - collection of text docs. VectorSource because we are using a column of the dataframe.
corpus = Corpus(VectorSource(data$Customer.Complaint))
#converting to lower case
corpus = tm_map(corpus, PlainTextDocument) #tm_map - Doing transformations in texts
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, removePunctuation)
#stop words - words that doesn't have much meaning. eg: is, are
corpus = tm_map(corpus, removeWords, c("and", stopwords("english")))
#Stemming - Find the root words
corpus = tm_map(corpus, stemDocument)
#Eliminate white spaces
corpus = tm_map(corpus, stripWhitespace)
#creating term document matrix
dtm <- TermDocumentMatrix(corpus) #Terms as rows and docs as columns
#inspect(dtm)
mat <- as.matrix(dtm) #make it a dense matrix
mat_sort <- sort(rowSums(mat),decreasing=TRUE)
ComplTypeCnt_mat <- data.frame(word = names(mat_sort),cnt=mat_sort) #created a dataframe with two columns - word and cnt
write.csv(ComplTypeCnt_mat,'./log/OutputFiles/ComplaintTypes_Freq.csv',row.names = F)
#Most used words
head(ComplTypeCnt_mat, 5)
#Create a wordcloud picture
set.seed(1234)  #for reproducing the same result
wordcloud(words = ComplTypeCnt_mat$word, freq = ComplTypeCnt_mat$cnt, random.order=TRUE,colors = brewer.pal(8,"Dark2"),scale=c(3.5,0.25)) #brewer.pal for R color palettes, scale for adjusting the pic
### Observations ###
log_print("By looking at Complaint Type Count matrix, we can conclude that most number of complaints are added in the categories of internet,service,bill and data.")
### Observations ###
# Q. Create a new categorical variable with value as Open and Closed.
# Open & Pending is to be categorized as Open and Closed & Solved is to be categorized as Closed.
data <- data %>%
mutate(Status_2 = case_when(
(Status == 'Open' | Status == 'Pending') ~ "Open",
(Status == 'Closed' | Status == 'Solved') ~ "Closed"
))
data$State <- tolower(data$State)  #To avoid duplicates
# Q. Provide state wise status of complaints in a stacked bar chart.
# group by state and status
data_grp_stateNStatus <- data %>%
group_by(State,Status_2) %>%
summarize(cnt = n_distinct(Customer.Complaint))
View(data_grp_stateNStatus)
data_grp_stateNStatus_pivot<-dcast(data_grp_stateNStatus, State ~ Status_2, value.var="cnt")
View(data_grp_stateNStatus_pivot)
Statewise_status<-dcast(data_grp_stateNStatus, State ~ Status_2, value.var="cnt")
data_grp_stateNStatus <- data %>%
group_by(State,Status_2) %>%
summarize(cnt = n_distinct(Customer.Complaint))
Statewise_status<-dcast(data_grp_stateNStatus, State ~ Status_2, value.var="cnt")
Statewise_status$Total <- Statewise_status$Closed + Statewise_status$Open
Statewise_status <- Statewise_status[order(Total),]
View(Statewise_status)
Statewise_status <- Statewise_status[order(Total),]
names(Statewise_status)
Statewise_status <- Statewise_status[order(Total),]
Statewise_status<-arrange(Statewise_status,Total)
?arrange
Statewise_status<-arrange(Statewise_status,desc(Total))
Statewise_status$State[0]
Statewise_status$State[1]
#Project 2 - comcast complaints analysis
### set working directory
#Can use this command if you have Rstudio IDE
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
### Libraries
library(logr)
library(lubridate) #for date parsing
library(dplyr)
library(ggplot2)  #visualization
library(reshape2) #pivot tble
#NLP packages
library(tm)  #Text mining package
#library(SnowballC) #collapsing word to common root-Stemming
library(RColorBrewer) #provide color schemes for maps
library(wordcloud)
### Set up log file
tmp <- file.path(dirname(rstudioapi::getActiveDocumentContext()$path), "SL_R_Proj3_Comcast.log")
# Open log
lf <- log_open(tmp)
# Send message to log
log_print("Simlilearn - DS with R Pgming - Project 2")
### Q.Import data in to R
data <- read.csv("../data/Comcast Telecom Complaints data.csv")
### To check missing values
#apply(is.na(data),2, which)
# No missing values found
### Functions
date_parsing <- function(dt) {
#print(dt)
if(!is.na(dt)){
for(date_format in c("%d/%m/%Y","%d-%m-%Y")){
datehead <- format(strptime(as.character(dt),date_format),"%Y-%m-%d")
if(is.na(datehead)){}
else{break}
}#end of for
}# end of if
return(datehead)
}
### Data cleaning - make dates in to same format
#str(data) # to get info about the dataframe columns. Here all cols are in chr format
data$newDate <- unlist(lapply(data$Date,date_parsing))
### Q.Plot data monthly
#create year month column
data$newDate <- as.Date(data$newDate)
#write.csv(data,'./log/temp.csv',row.names = F)
#group by month
data_grp_mth <- data %>%
group_by(month = lubridate::floor_date(newDate, "month")) %>%
summarize(cnt = n_distinct(Customer.Complaint))
#plot data
ggplot(data_grp_mth, aes(month, cnt)) +
geom_point() +
geom_line() +
xlab("Month") +
ylab("Number of Complaints") +
ggtitle("Monthly Comcast complaint frequency")
### Q.Plot data daily
#group by daily
data_grp_day <- data %>%
group_by(newDate) %>%
summarize(cnt = n_distinct(Customer.Complaint))
#plot data
ggplot(data_grp_day, aes(newDate, cnt)) +
geom_point() +
geom_line() +
xlab("Date") +
ylab("Number of Complaints") +
ggtitle("Daily Comcast complaint frequency")
### Observations ###
log_print("By looking at the daily and monthly plots, we can conclude that most number of complaints are added in June 2015.")
### Observations ###
### Q.Provide a table with the frequency of complaint types.
### Q.Which complaint types are maximum i.e., around internet, network issues, or across any other domains.
#Identify different complaint types from Customer.complaint column values and their count
#corpus - collection of text docs. VectorSource because we are using a column of the dataframe.
corpus = Corpus(VectorSource(data$Customer.Complaint))
#converting to lower case
corpus = tm_map(corpus, PlainTextDocument) #tm_map - Doing transformations in texts
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, removePunctuation)
#stop words - words that doesn't have much meaning. eg: is, are
corpus = tm_map(corpus, removeWords, c("and", stopwords("english")))
#Stemming - Find the root words
corpus = tm_map(corpus, stemDocument)
#Eliminate white spaces
corpus = tm_map(corpus, stripWhitespace)
#creating term document matrix
dtm <- TermDocumentMatrix(corpus) #Terms as rows and docs as columns
#inspect(dtm)
mat <- as.matrix(dtm) #make it a dense matrix
mat_sort <- sort(rowSums(mat),decreasing=TRUE)
ComplTypeCnt_mat <- data.frame(word = names(mat_sort),cnt=mat_sort) #created a dataframe with two columns - word and cnt
write.csv(ComplTypeCnt_mat,'./log/OutputFiles/ComplaintTypes_Freq.csv',row.names = F)
#Most used words
head(ComplTypeCnt_mat, 5)
#Create a wordcloud picture
set.seed(1234)  #for reproducing the same result
wordcloud(words = ComplTypeCnt_mat$word, freq = ComplTypeCnt_mat$cnt, random.order=TRUE,colors = brewer.pal(8,"Dark2"),scale=c(3.5,0.25)) #brewer.pal for R color palettes, scale for adjusting the pic
### Observations ###
log_print("By looking at Complaint Type Count matrix, we can conclude that most number of complaints are added in the categories of internet,service,bill and data.")
### Observations ###
# Q. Create a new categorical variable with value as Open and Closed.
# Open & Pending is to be categorized as Open and Closed & Solved is to be categorized as Closed.
data <- data %>%
mutate(Status_2 = case_when(
(Status == 'Open' | Status == 'Pending') ~ "Open",
(Status == 'Closed' | Status == 'Solved') ~ "Closed"
))
data$State <- tolower(data$State)  #To avoid duplicates
# Q. Provide state wise status of complaints in a stacked bar chart.
#plot data
ggplot(data = data, aes(y = State)) +
geom_bar(aes(fill = Status_2)) +
ggtitle("State wise status of complaints ")
data_grp_stateNStatus <- data %>%
group_by(State,Status_2) %>%
summarize(cnt = n_distinct(Customer.Complaint))
Statewise_status<-dcast(data_grp_stateNStatus, State ~ Status_2, value.var="cnt")
Statewise_status$Total <- Statewise_status$Closed + Statewise_status$Open
Statewise_status<-arrange(Statewise_status,desc(Total))
log_print(paste(Statewise_status$State[1], "state has maximum number of complaints.",sep = " "))
View(data_grp_mth)
data_grp_mth<-arrange(data_grp_mth,desc(cnt))
max_month <- format(data_grp_mth$month[1],"%B-%Y")
max_month
log_print(paste("By looking at the daily and monthly plots, we can conclude that most number of complaints are added in ",max_month,".",sep = ""))
View(ComplTypeCnt_mat)
log_print(paste("By looking at Complaint Type Count matrix, we can conclude that most number of complaints are added in the categories of ",ComplTypeCnt_mat$word[1],ComplTypeCnt_mat$word[2],ComplTypeCnt_mat$word[3],ComplTypeCnt_mat$word[4],ComplTypeCnt_mat$word[5],ComplTypeCnt_mat$word[6],sep=","))
log_print(paste("By looking at Complaint Type Count matrix, we can conclude that most number of complaints are added in the categories of:",ComplTypeCnt_mat$word[1],ComplTypeCnt_mat$word[2],ComplTypeCnt_mat$word[3],ComplTypeCnt_mat$word[4],ComplTypeCnt_mat$word[5],ComplTypeCnt_mat$word[6],sep=","))
View(Statewise_status)
Statewise_status$UnresolvedPerc <- (Statewise_status$Total / Statewise_status$Open)*100
Statewise_status<-arrange(Statewise_status,desc(UnresolvedPerc))
data_grp_stateNStatus <- data %>%
group_by(State,Status_2) %>%
summarize(cnt = n_distinct(Customer.Complaint))
Statewise_status<-dcast(data_grp_stateNStatus, State ~ Status_2, value.var="cnt")
Statewise_status$Total <- Statewise_status$Closed + Statewise_status$Open
Statewise_status<-arrange(Statewise_status,desc(Total))
Statewise_status$UnresolvedPerc <- (Statewise_status$Open / Statewise_status$Total)*100
Statewise_status<-arrange(Statewise_status,desc(UnresolvedPerc))
Statewise_status<-arrange(Statewise_status,desc(UnresolvedPerc))
str_to_title(Statewise_status$State[1])
library(stringr)
str_to_title(Statewise_status$State[1])
data_grp_stateNStatus <- data %>%
group_by(State,Status_2) %>%
summarize(cnt = n_distinct(Customer.Complaint))
Statewise_status<-dcast(data_grp_stateNStatus, State ~ Status_2, value.var="cnt")
Statewise_status$Total <- Statewise_status$Closed + Statewise_status$Open
Statewise_status<-arrange(Statewise_status,desc(Total))
log_print(paste(str_to_title(Statewise_status$State[1]), "state has maximum number of complaints.",sep = " "),hide_notes = TRUE)
#  - Which state has the highest percentage of unresolved complaints
Statewise_status$UnresolvedPerc <- (Statewise_status$Open / Statewise_status$Total)*100
Statewise_status_unres<-arrange(Statewise_status,desc(UnresolvedPerc))
View(Statewise_status_unres)
log_print(paste(str_to_title(Statewise_status_unres$State[1]),"has highest percentage of unresolved complaints.",sep = " "),hide_notes = TRUE)
log_print("In this data set, Kansas has higher unresolved rate. However, it only got 1 Open complaint and 1 Unresolved.That is why it got 50% unresolved rate. So this result doesn't mean much. I am not entirely sure whether this is what we want.",hide_notes = TRUE)
View(Data)
View(data)
View(data)
data_grp_res <- data %>%
group_by(Received.Via,Status_2) %>%
summarize(cnt = n_distinct(Customer.Complaint))
view(data_grp_res)
View(data_grp_res)
data_grp_res$perc <-  (data_grp_res$cnt/nrow(data)) * 100
data_resPerc_call <- filter(data_grp_res,Received.Via=="Customer Care Call" & Status_2=="Closed")
data_resPerc_call
data_resPerc_call$perc[1]
data_resPerc_Internet <- filter(data_grp_res,Received.Via=="Internet" & Status_2=="Closed")
log_print(paste("The resolved rate for Internet is:",data_resPerc_Internet$perc[1],sep = ""),hide_notes = TRUE )
source("~/Repositories/GitHubRepos/Data_Science/SL_R/SL_R_Proj2_Comcast.R")
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
getwd()
str(data)
summary(data)
head(data)
data<- read.csv("Data.csv")
getwd()
ls
getwd()
list.files()
list.files()
data<- read.csv("Data.csv")
str(data)
summary(data)
head(data)
library(psych)
psych::describe(data)
describe(data)
boxplot(data)
boxplot(data$Price_house)
data2 <- data[data$Price_house <10198905, ]
data2 <- data[data$Price_house <10198905, ]
boxplot(data$Price_house)
data2 <- data[data$Price_house <10198905, ]
boxplot(data2$Price_house)
data2 <- data[data$Price_house <10198905, ]
boxplot(data2$Price_house)
data3 <- data2[data2$Price_house > 30000, ]
boxplot(data3$Price_house)
library(corrgram)
install.packages("corrgram")
install.packages("psych")
library(psych)
psych::describe(data)
x=corrgram(data, order=TRUE) #
library(corrgram)  #to show correlation matrix
x=corrgram(data, order=TRUE)
x
sapply(data, function(x) sum(is.na(x)))
data <- na.omit(data) #delete the missing rows
str(data)
fit<- lm(Price_house ~ Taxi_dist    +	Market_dist    +	Hospital_dist    +	Carpet_area
+	Builtup_area    +	Parking_type    +	City_type    +	Rainfall
, data=data)
summary(fit)
str(dat)
str(data)
tt <- sort(table(c("a", "b", "a", "a", "b", "c", "a1", "a1", "a1")), dec=T)
tt
x = c(1,56,8)
y = c(8,2,11,8)
x*y
for (year in 1:5)
{
Yr=print(year)
}
z <- 0
while(z < 5){
z <- z + 2
print(z)
}
DataFrame1=data.frame(v1 = c(2,4,12,3,6))
max(DataFrame1$v1)
LogicalVector = c(TRUE,FALSE,0,1)
class(LogicalVector)
m = matrix(1:6,nrow=2,ncol=3,byrow=TRUE)
m
rep(1:10,2)
x <-1:9
y <-15:17
cbind(x, y)[1,]
x = 1 y=2
list(5,"John",TRUE,1+ 9i)
x = list(78,"Tim",101,8i)
length(x)
x = as.character(factor(c("No", "yes", "no", "yes", "no")))
class(x)
sum(2,8,9,NA)
c(T,F,TRUE,1)
EmpRecs = data.frame(EmpID = c(101,102,103), Name = c("John","Theresa","Andy","Paul"))
sum(2,8,9,NA,na.rm=T)
for(i in seq(from=1,to=10,by=2))
{
Variable1=print(i)
}
x <-1
switch(x, 2+2, sum(1:10), max(1:10))
x = c(51,93,8,67,22)
x[x>50 & x<90]
apply(iris[,1:4],2,mean)
apply(iris[,1:4],1,mean)
